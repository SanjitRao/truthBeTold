#!/bin/bash

python preserve_only_numeric_and_ner.py dev
python preserve_only_numeric_and_ner.py train

cat data/filtered/train/source.txt.txt \
    data/filtered/train/target.txt.txt \
    | seq2seq/bin/tools/generate_vocab.py > data/filtered/vocab

head -n 30000 data/filtered/vocab > data/filtered/vocab-30000

export VOCAB_SOURCE=data/filtered/vocab-30000
export VOCAB_TARGET=data/filtered/vocab-30000
export TRAIN_SOURCES=data/filtered/train/source.txt.txt
export TRAIN_TARGETS=data/filtered/train/target.txt.txt
export DEV_SOURCES=data/filtered/dev/source.txt.txt
export DEV_TARGETS=data/filtered/dev/target.txt.txt
export DEV_TARGETS_REF=data/filtered/dev/target.txt.txt
export TRAIN_STEPS=50000

export MODEL_DIR=$(pwd)/tmp-filtered/
mkdir -p $MODEL_DIR

python -m bin.train \
  --config_paths="
      ./seq2seq/example_configs/reverse_squad.yml,
      ./seq2seq/example_configs/train_seq2seq.yml,
      ./seq2seq/example_configs/text_metrics_raw_bleu.yml" \
  --model_params "
      vocab_source: $VOCAB_SOURCE
      vocab_target: $VOCAB_TARGET" \
  --input_pipeline_train "
    class: ParallelTextInputPipeline
    params:
      source_files:
        - $TRAIN_SOURCES
      target_files:
        - $TRAIN_TARGETS" \
  --input_pipeline_dev "
    class: ParallelTextInputPipeline
    params:
       source_files:
        - $DEV_SOURCES
       target_files:
        - $DEV_TARGETS" \
  --batch_size 64 \
  --train_steps $TRAIN_STEPS \
  --output_dir $MODEL_DIR \
  --eval_every_n_steps=1000 \
  --keep_checkpoint_max=50
